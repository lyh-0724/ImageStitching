{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1141c51-40dd-4c81-8c9f-11733aad9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from math import log10\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_ssim\n",
    "from data_utils import TrainDatasetFromFolder, ValDatasetFromFolder, display_transform\n",
    "from loss import GeneratorLoss\n",
    "from model import Generator, Discriminator\n",
    "\n",
    "# 创建一个命令行参数解析器对象\n",
    "parser = argparse.ArgumentParser(description='Train Super Resolution Models')\n",
    "# 用于指定训练图像的裁剪尺寸，默认为88\n",
    "parser.add_argument('--crop_size', default=88, type=int, help='training images crop size')\n",
    "# 用于指定超分辨率的放大因子，默认为4\n",
    "parser.add_argument('--upscale_factor', default=4, type=int, choices=[2, 4, 8],\n",
    "                    help='super resolution upscale factor')\n",
    "# 用于指定训练的轮数，默认为100\n",
    "parser.add_argument('--num_epochs', default=100, type=int, help='train epoch number')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 解析命令行参数并将结果存储在变量opt中\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    # 从opt中获取crop_size、upscale_factor和num_epochs的值，并分别赋给对应的变量\n",
    "    CROP_SIZE = opt.crop_size\n",
    "    UPSCALE_FACTOR = opt.upscale_factor\n",
    "    NUM_EPOCHS = opt.num_epochs\n",
    "\n",
    "    # 创建训练数据集对象TrainDatasetFromFolder，指定数据集路径、裁剪尺寸和放大因子\n",
    "    train_set = TrainDatasetFromFolder('data/VOC2012/train', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "    # 创建验证数据集对象ValDatasetFromFolder，指定数据集路径和放大因子\n",
    "    val_set = ValDatasetFromFolder('data/VOC2012/val', upscale_factor=UPSCALE_FACTOR)\n",
    "    # 创建训练数据加载器，指定数据集对象、工作线程数、批量大小和是否打乱数据顺序\n",
    "    train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=64, shuffle=True)\n",
    "    # 创建验证数据加载器，指定数据集对象、工作线程数、批量大小和是否打乱数据顺序\n",
    "    val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)\n",
    "\n",
    "    # 创建生成器模型对象Generator，指定放大因子\n",
    "    netG = Generator(UPSCALE_FACTOR)\n",
    "    # 输出生成器模型参数的数量\n",
    "    print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
    "    # 创建生成器损失函数对象GeneratorLoss\n",
    "    netD = Discriminator()\n",
    "    # 输出判别器模型参数的数量\n",
    "    print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
    "\n",
    "    # 创建生成器损失函数对象GeneratorLoss\n",
    "    generator_criterion = GeneratorLoss()\n",
    "\n",
    "    # GPU如果可用的话，将生成器模型、判别器模型和生成器损失函数移动到GPU上进行计算\n",
    "    if torch.cuda.is_available():\n",
    "        netG.cuda()\n",
    "        netD.cuda()\n",
    "        generator_criterion.cuda()\n",
    "\n",
    "    # 创建生成器和判别器的优化器对象，用于更新模型参数\n",
    "    optimizerG = optim.Adam(netG.parameters())\n",
    "    optimizerD = optim.Adam(netD.parameters())\n",
    "\n",
    "    # 创建一个字典用于存储训练过程中的判别器和生成器的损失、分数和评估指标结果(信噪比和相似性)\n",
    "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        # 创建训练数据的进度条\n",
    "        train_bar = tqdm(train_loader)\n",
    "        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "    \n",
    "        netG.train()  # 将生成器设置为训练模式\n",
    "        netD.train()  # 将判别器设置为训练模式\n",
    "        for data, target in train_bar:\n",
    "            g_update_first = True\n",
    "            batch_size = data.size(0)\n",
    "            running_results['batch_sizes'] += batch_size\n",
    "\n",
    "            # (1) Update D network: maximize D(x)-1-D(G(z))\n",
    "            real_img = Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                real_img = real_img.cuda()\n",
    "            z = Variable(data)\n",
    "            if torch.cuda.is_available():\n",
    "                z = z.cuda()\n",
    "            fake_img = netG(z)  # 通过生成器生成伪图像\n",
    "\n",
    "            # 清除判别器的梯度\n",
    "            netD.zero_grad()\n",
    "            # 通过判别器对真实图像进行前向传播，并计算其输出的平均值\n",
    "            real_out = netD(real_img).mean()\n",
    "            # 通过判别器对伪图像进行前向传播，并计算其输出的平均值\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            # 计算判别器的损失\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "            # 在判别器网络中进行反向传播，并保留计算图以进行后续优化步骤\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            # 利用优化器对判别器网络的参数进行更新\n",
    "            optimizerD.step()\n",
    "\n",
    "            # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "            netG.zero_grad()\n",
    "            # The two lines below are added to prevent runtime error in Google Colab\n",
    "            # 通过生成器对输入图像（z）进行生成，生成伪图像（fake_img）\n",
    "            fake_img = netG(z)\n",
    "            # 通过判别器对伪图像进行前向传播，并计算其输出的平均值\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            ##\n",
    "            # 计算生成器的损失，包括对抗损失、感知损失、图像损失和TV损失\n",
    "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "            # 在生成器网络中进行反向传播，计算生成器的梯度\n",
    "            g_loss.backward()\n",
    "\n",
    "            # 再次通过生成器对输入图像（z）进行生成，得到新的伪图像（fake_img）\n",
    "            fake_img = netG(z)\n",
    "            # 通过判别器对新的伪图像进行前向传播，并计算其输出的平均值\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            # 利用优化器对生成器网络的参数进行更新\n",
    "            optimizerG.step()\n",
    "\n",
    "            # loss for current batch before optimization\n",
    "            # 累加当前批次生成器的损失值乘以批次大小，用于计算平均损失\n",
    "            running_results['g_loss'] += g_loss.item() * batch_size\n",
    "            # 累加当前批次判别器的损失值乘以批次大小，用于计算平均损失\n",
    "            running_results['d_loss'] += d_loss.item() * batch_size\n",
    "            # 累加当前批次真实图像在判别器的输出得分乘以批次大小，用于计算平均得分\n",
    "            running_results['d_score'] += real_out.item() * batch_size\n",
    "            # 累加当前批次伪图像在判别器的输出得分乘以批次大小，用于计算平均得分\n",
    "            running_results['g_score'] += fake_out.item() * batch_size\n",
    "\n",
    "            # 更新训练进度条的描述信息\n",
    "            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "                running_results['g_loss'] / running_results['batch_sizes'],\n",
    "                running_results['d_score'] / running_results['batch_sizes'],\n",
    "                running_results['g_score'] / running_results['batch_sizes']))\n",
    "\n",
    "\n",
    "        netG.eval()\n",
    "        # 创建用于保存训练结果的目录\n",
    "        out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader)\n",
    "            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "            val_images = []\n",
    "            # 遍历验证数据集(低分辨率图 恢复的高分辨率图 高分辨率图)\n",
    "            for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "                batch_size = val_lr.size(0)\n",
    "                valing_results['batch_sizes'] += batch_size\n",
    "                lr = val_lr\n",
    "                hr = val_hr\n",
    "                if torch.cuda.is_available():\n",
    "                    lr = lr.cuda()\n",
    "                    hr = hr.cuda()\n",
    "\n",
    "                # 生成超分辨率图像\n",
    "                sr = netG(lr)\n",
    "\n",
    "                # 计算批量图像的均方误差\n",
    "                batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "                # 累加均方误差\n",
    "                valing_results['mse'] += batch_mse * batch_size\n",
    "                # 计算批量图像的结构相似度指数\n",
    "                batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "                # 累加结构相似度指数\n",
    "                valing_results['ssims'] += batch_ssim * batch_size\n",
    "                # 计算平均峰值信噪比\n",
    "                valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "                # 计算平均结构相似度指数\n",
    "                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "                # 更新训练进度条的描述信息\n",
    "                val_bar.set_description(\n",
    "                    desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                        valing_results['psnr'], valing_results['ssim']))\n",
    "        \n",
    "                val_images.extend(\n",
    "                    # 将图像应用转换函数，并添加到验证图像列表\n",
    "                    [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
    "                     display_transform()(sr.data.cpu().squeeze(0))])\n",
    "\n",
    "            # 将验证图像列表堆叠为张量\n",
    "            val_images = torch.stack(val_images)\n",
    "            # 将堆叠后的张量分割为多个小块，每个小块包含15张图像\n",
    "            val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
    "            # 创建进度条，并设置描述为“[saving training results]”\n",
    "            val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
    "            index = 1\n",
    "            for image in val_save_bar:\n",
    "                # 将小块中的图像创建为一个网格，每行显示3张图像，图像之间有5个像素的间隔\n",
    "                image = utils.make_grid(image, nrow=3, padding=5)\n",
    "                # 将网格图像保存为文件，文件名包含epoch和index信息\n",
    "                utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
    "                index += 1\n",
    "    \n",
    "        # save model parameters\n",
    "        # 将判别器和生成器的参数保存到指定文件\n",
    "        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "\n",
    "        # save loss\\scores\\psnr\\ssim\n",
    "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "        results['psnr'].append(valing_results['psnr'])\n",
    "        results['ssim'].append(valing_results['ssim'])\n",
    "    \n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            out_path = 'statistics/'\n",
    "            # 创建一个DataFrame对象，用于存储训练结果数据\n",
    "            data_frame = pd.DataFrame(\n",
    "                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "                index=range(1, epoch + 1))\n",
    "            # 将DataFrame对象保存为CSV文件\n",
    "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
